[
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "preprocessing",
        "importPath": "sklearn",
        "description": "sklearn",
        "isExtraImport": true,
        "detail": "sklearn",
        "documentation": {}
    },
    {
        "label": "Perceptron",
        "kind": 6,
        "importPath": "T02.Perceptron",
        "description": "T02.Perceptron",
        "peekOfCode": "class Perceptron:\n    # Constructor\n    def __init__(self, n_input, learning_rate) -> None:\n        # Inicializamos las variables\n        self.w = -1 + 2 * np.random.rand(n_input)\n        self.b = -1 + 2 * np.random.rand()\n        self.eta = learning_rate\n    # Funcion de prediccion\n    def predict(self, X) -> bool:\n        _, p = X.shape",
        "detail": "T02.Perceptron",
        "documentation": {}
    },
    {
        "label": "draw_2d",
        "kind": 2,
        "importPath": "T02.Perceptron",
        "description": "T02.Perceptron",
        "peekOfCode": "def draw_2d(model, i, last = False) -> None:\n    # variables a utilizar\n    w1, w2, b = model.w[0], model.w[1], model.b\n    li, ls = -2, 2 # limite inferior y sulperior\n    if ( last ):\n        plt.plot(\n            [li, ls],\n            [(1/w2)*(-w1*(li)-b), (1/w2)*(-w1*(ls)-b)],\n            '--k', label='last'\n        )",
        "detail": "T02.Perceptron",
        "documentation": {}
    },
    {
        "label": "get_normalized",
        "kind": 2,
        "importPath": "T02.Perceptron",
        "description": "T02.Perceptron",
        "peekOfCode": "def get_normalized(N):\n    Y = np.zeros(N)\n    weight_lower, weight_upper = 35, 120 # Establecemos limites de pesos\n    height_lower, height_upper = 1.2, 2.1 # Y limite par las alturas\n    # Generamos los pesos y las alturas de manera aleatoria\n    X = np.array([\n        (weight_lower + (weight_upper - weight_lower) * np.random.rand(N)),\n        (height_lower + (height_upper - height_lower) * np.random.rand(N))\n    ])\n    # Salida",
        "detail": "T02.Perceptron",
        "documentation": {}
    },
    {
        "label": "LinearNeuron",
        "kind": 6,
        "importPath": "T03.LinearNeuron",
        "description": "T03.LinearNeuron",
        "peekOfCode": "class LinearNeuron:\n    # Constructor\n    def __init__(self, n_inputs, learning_rate=0.1) -> None:\n        # Inicializamos las variables\n        self.w = -1+2*np.random.rand( n_inputs )\n        self.b = -1+2*np.random.rand()\n        self.eta = learning_rate\n    # funcion prediccion\n    def predict(self, X):\n        Y_est = np.dot(self.w, X) + self.b",
        "detail": "T03.LinearNeuron",
        "documentation": {}
    },
    {
        "label": "LogisticNeuron",
        "kind": 6,
        "importPath": "T04.LogisticNeuron",
        "description": "T04.LogisticNeuron",
        "peekOfCode": "class LogisticNeuron:\n    # Atributos:\n    # w = array de pesos\n    # b = sesgo\n    # eta = taza de aprendizaje utilizado durante el entrenamiento\n    def __init__( self, n_inputs, learning_rate=0.1 ) -> None:\n        self.w = -1 + 2 * np.random.rand( n_inputs )\n        self.b = -1 + 2 * np.random.rand()\n        self.eta = learning_rate\n    def predict_proba( self, X ):",
        "detail": "T04.LogisticNeuron",
        "documentation": {}
    },
    {
        "label": "OLN",
        "kind": 6,
        "importPath": "T05.softmax",
        "description": "T05.softmax",
        "peekOfCode": "class OLN:\n    ## constructor\n    def __init__(self, number_inputs, number_outputs, activation_function = linear):\n        self.w = -1 + 2*np.random.rand(number_outputs, number_inputs)\n        self.b = -1 + 2*np.random.rand(number_outputs, 1)\n        self.f = activation_function\n    def predict(self, X):\n        Z = self.w @ X + self.b # Multiplicacion matricial\n        return self.f(Z)\n    def fit(self, X, Y, epochs=500, learning_rate=0.1):",
        "detail": "T05.softmax",
        "documentation": {}
    },
    {
        "label": "linear",
        "kind": 2,
        "importPath": "T05.softmax",
        "description": "T05.softmax",
        "peekOfCode": "def linear(z, derivate=False):\n    a = z\n    if derivate:\n        da = np.ones(z.shape)\n        return a, da\n    return a\n## neurona logistica\ndef logistic(z, derivate=False):\n    a = 1/(1 + np.exp(-z))\n    if derivate:",
        "detail": "T05.softmax",
        "documentation": {}
    },
    {
        "label": "logistic",
        "kind": 2,
        "importPath": "T05.softmax",
        "description": "T05.softmax",
        "peekOfCode": "def logistic(z, derivate=False):\n    a = 1/(1 + np.exp(-z))\n    if derivate:\n        da = np.ones(z.shape, dtype=float)\n        return a, da\n    return a\n## softmax\ndef softmax(z, derivate=False):\n    e_z = np.exp(z-np.max(z, axis=0))\n    a = e_z/np.sum(e_z, axis=0)",
        "detail": "T05.softmax",
        "documentation": {}
    },
    {
        "label": "softmax",
        "kind": 2,
        "importPath": "T05.softmax",
        "description": "T05.softmax",
        "peekOfCode": "def softmax(z, derivate=False):\n    e_z = np.exp(z-np.max(z, axis=0))\n    a = e_z/np.sum(e_z, axis=0)\n    if derivate:\n        da = np.ones(z.shape)\n        return a, da\n    return a\n## One Layer Network\nclass OLN:\n    ## constructor",
        "detail": "T05.softmax",
        "documentation": {}
    },
    {
        "label": "plot_data",
        "kind": 2,
        "importPath": "T05.softmax",
        "description": "T05.softmax",
        "peekOfCode": "def plot_data(X, Y, net):\n    dot_c = ('red', 'green', 'yellow', 'black')\n    lin_c = ('-r', '-g', '-y', '-k')\n    for i in range(X.shape[1]):\n        c = np.argmax(Y[:, i])\n        plt.scatter(X[0, i], X[1, i], color=dot_c[c], edgecolor='k')\n    for i in range(4):\n        w1, w2, b = net.w[i, 0], net.w[i,1], net.b[i]\n        plt.plot([-0.5, 1.5], [(1/w2)*(-w1*(-0.5)-b), (1/w2)*(-w1*(1.5)-b)], lin_c[i])\n    ## limites para el ploteo",
        "detail": "T05.softmax",
        "documentation": {}
    },
    {
        "label": "plot_data_softmax",
        "kind": 2,
        "importPath": "T05.softmax",
        "description": "T05.softmax",
        "peekOfCode": "def plot_data_softmax(X, Y, net):\n    dot_c = ('red', 'green', 'yellow', 'black')\n    xx, yy = np.meshgrid(np.linspace(0,1,100), np.linspace(0,1,100))\n    data = [xx.ravel(), yy.ravel()]\n    zz = net.predict(data)\n    zz = np.argmax(zz, axis=0)\n    zz = zz.reshape(xx.shape)\n    plt.contourf(xx, yy, zz, alpha=0.8, cmap=plt.get_cmap('Set1'))\n    for i in range(X.shape[1]):\n        c = np.argmax(Y[:, i])",
        "detail": "T05.softmax",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "T05.softmax",
        "description": "T05.softmax",
        "peekOfCode": "df = pd.read_csv('Dataset_A05.csv') # leemos los datos del csv\n## Tratamos los datos para que conicidan\nX = np.asanyarray(df.iloc[:, :2]).T\nY = np.asanyarray(df.iloc[:, 2:]).T\nnet = OLN(2, 4, logistic) # [ 'logistic', 'linear' ]\nnet.fit(X, Y, epochs=1000, learning_rate=1)\nplot_data(X, Y, net)\n## Parte 2\nnet = OLN(2, 4, softmax)\nnet.fit(X, Y, epochs=10000, learning_rate=1)",
        "detail": "T05.softmax",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "T05.softmax",
        "description": "T05.softmax",
        "peekOfCode": "X = np.asanyarray(df.iloc[:, :2]).T\nY = np.asanyarray(df.iloc[:, 2:]).T\nnet = OLN(2, 4, logistic) # [ 'logistic', 'linear' ]\nnet.fit(X, Y, epochs=1000, learning_rate=1)\nplot_data(X, Y, net)\n## Parte 2\nnet = OLN(2, 4, softmax)\nnet.fit(X, Y, epochs=10000, learning_rate=1)\ndef plot_data_softmax(X, Y, net):\n    dot_c = ('red', 'green', 'yellow', 'black')",
        "detail": "T05.softmax",
        "documentation": {}
    },
    {
        "label": "Y",
        "kind": 5,
        "importPath": "T05.softmax",
        "description": "T05.softmax",
        "peekOfCode": "Y = np.asanyarray(df.iloc[:, 2:]).T\nnet = OLN(2, 4, logistic) # [ 'logistic', 'linear' ]\nnet.fit(X, Y, epochs=1000, learning_rate=1)\nplot_data(X, Y, net)\n## Parte 2\nnet = OLN(2, 4, softmax)\nnet.fit(X, Y, epochs=10000, learning_rate=1)\ndef plot_data_softmax(X, Y, net):\n    dot_c = ('red', 'green', 'yellow', 'black')\n    xx, yy = np.meshgrid(np.linspace(0,1,100), np.linspace(0,1,100))",
        "detail": "T05.softmax",
        "documentation": {}
    },
    {
        "label": "net",
        "kind": 5,
        "importPath": "T05.softmax",
        "description": "T05.softmax",
        "peekOfCode": "net = OLN(2, 4, logistic) # [ 'logistic', 'linear' ]\nnet.fit(X, Y, epochs=1000, learning_rate=1)\nplot_data(X, Y, net)\n## Parte 2\nnet = OLN(2, 4, softmax)\nnet.fit(X, Y, epochs=10000, learning_rate=1)\ndef plot_data_softmax(X, Y, net):\n    dot_c = ('red', 'green', 'yellow', 'black')\n    xx, yy = np.meshgrid(np.linspace(0,1,100), np.linspace(0,1,100))\n    data = [xx.ravel(), yy.ravel()]",
        "detail": "T05.softmax",
        "documentation": {}
    },
    {
        "label": "net",
        "kind": 5,
        "importPath": "T05.softmax",
        "description": "T05.softmax",
        "peekOfCode": "net = OLN(2, 4, softmax)\nnet.fit(X, Y, epochs=10000, learning_rate=1)\ndef plot_data_softmax(X, Y, net):\n    dot_c = ('red', 'green', 'yellow', 'black')\n    xx, yy = np.meshgrid(np.linspace(0,1,100), np.linspace(0,1,100))\n    data = [xx.ravel(), yy.ravel()]\n    zz = net.predict(data)\n    zz = np.argmax(zz, axis=0)\n    zz = zz.reshape(xx.shape)\n    plt.contourf(xx, yy, zz, alpha=0.8, cmap=plt.get_cmap('Set1'))",
        "detail": "T05.softmax",
        "documentation": {}
    },
    {
        "label": "DenseNetwork",
        "kind": 6,
        "importPath": "T06.MLP",
        "description": "T06.MLP",
        "peekOfCode": "class DenseNetwork:\n    def __init__(self, layers_dim, hidden_activation=tanh, output_activation=logistic):\n        # Atrubutos\n        self.L = len(layers_dim) - 1\n        self.w = [ None ] * ( self.L + 1 )\n        self.b = [ None ] * ( self.L + 1 )\n        self.f = [ None ] * ( self.L + 1 )\n        # Inicializamos los pesos y los sesgos\n        for l in range( 1, self.L + 1):\n            self.w[l] = -1 + 2 * np.random.rand(layers_dim[l], layers_dim[l-1])",
        "detail": "T06.MLP",
        "documentation": {}
    },
    {
        "label": "linear",
        "kind": 2,
        "importPath": "T06.MLP",
        "description": "T06.MLP",
        "peekOfCode": "def linear(z, derivative=False):\n    a = z\n    if derivative:\n        da = np.ones(z.shape)\n        return a, da\n    return a\ndef logistic(z, derivative=False):\n    a = 1 / (1+np.exp(-z))\n    if derivative:\n        da = np.ones(z.shape)",
        "detail": "T06.MLP",
        "documentation": {}
    },
    {
        "label": "logistic",
        "kind": 2,
        "importPath": "T06.MLP",
        "description": "T06.MLP",
        "peekOfCode": "def logistic(z, derivative=False):\n    a = 1 / (1+np.exp(-z))\n    if derivative:\n        da = np.ones(z.shape)\n        return a, da\n    return a\ndef logistic_hidden(z, derivative=False):\n    a = 1 / (1+np.exp(-z))\n    if derivative:\n        da = a * (1-a)",
        "detail": "T06.MLP",
        "documentation": {}
    },
    {
        "label": "logistic_hidden",
        "kind": 2,
        "importPath": "T06.MLP",
        "description": "T06.MLP",
        "peekOfCode": "def logistic_hidden(z, derivative=False):\n    a = 1 / (1+np.exp(-z))\n    if derivative:\n        da = a * (1-a)\n        return a, da\n    return a\ndef tanh(z, derivative=False):\n    a = np.tanh(z)\n    if derivative:\n        da = (1+a) * (1-a)",
        "detail": "T06.MLP",
        "documentation": {}
    },
    {
        "label": "tanh",
        "kind": 2,
        "importPath": "T06.MLP",
        "description": "T06.MLP",
        "peekOfCode": "def tanh(z, derivative=False):\n    a = np.tanh(z)\n    if derivative:\n        da = (1+a) * (1-a)\n        return a, da\n    return a\ndef relu(z, derivative=False):\n    a = z * (z>=0)\n    if derivative:\n        da = np.array(z >= 0, dtype=float)",
        "detail": "T06.MLP",
        "documentation": {}
    },
    {
        "label": "relu",
        "kind": 2,
        "importPath": "T06.MLP",
        "description": "T06.MLP",
        "peekOfCode": "def relu(z, derivative=False):\n    a = z * (z>=0)\n    if derivative:\n        da = np.array(z >= 0, dtype=float)\n        return a, da\n    return a\ndef softmax(z, derivative=False):\n    e_z = np.exp(z - np.max(z, axis=0))\n    a = e_z / np.sum(e_z, axis=0)\n    if derivative:",
        "detail": "T06.MLP",
        "documentation": {}
    },
    {
        "label": "softmax",
        "kind": 2,
        "importPath": "T06.MLP",
        "description": "T06.MLP",
        "peekOfCode": "def softmax(z, derivative=False):\n    e_z = np.exp(z - np.max(z, axis=0))\n    a = e_z / np.sum(e_z, axis=0)\n    if derivative:\n        da = np.ones(z.shape)\n        return a, da\n    return a\n## Clase MLP\nclass DenseNetwork:\n    def __init__(self, layers_dim, hidden_activation=tanh, output_activation=logistic):",
        "detail": "T06.MLP",
        "documentation": {}
    },
    {
        "label": "MLP_binary_classification_2d",
        "kind": 2,
        "importPath": "T06.MLP",
        "description": "T06.MLP",
        "peekOfCode": "def MLP_binary_classification_2d( subplot, X, Y, net ):\n    for i in range(X.shape[1]):\n        if Y[i]==0:\n            subplot.plot(X[0,i], X[1,i], 'ro', markersize=5)\n        else:\n            subplot.plot(X[0,i], X[1,i], 'bo',markersize=5)\n    xmin, ymin=np.min(X[0,:])-0.5, np.min(X[1,:])-0.5\n    xmax, ymax=np.max(X[0,:])+0.5, np.max(X[1,:])+0.5\n    xx, yy = np.meshgrid(np.linspace(xmin,xmax, 100), np.linspace(ymin,ymax, 100))\n    data = [xx.ravel(), yy.ravel()]",
        "detail": "T06.MLP",
        "documentation": {}
    },
    {
        "label": "neurons",
        "kind": 5,
        "importPath": "T06.MLP",
        "description": "T06.MLP",
        "peekOfCode": "neurons = 10 # Mover el numero de neuronas segun desee\nnet = DenseNetwork((2, neurons, 1)) # Definimos que tenemos 2 entradas y 1 salida\n# Elegir un csv  -->  [ 'moons', 'blobs', 'circles', 'XOR' ]\nX = np.genfromtxt(\"XOR.csv\", delimiter=',', skip_header=1, usecols=[0,1]).T\nY = np.genfromtxt(\"XOR.csv\", delimiter=',', skip_header=1, usecols=[2]).T\nprint(X.shape, Y.shape)\nplt.figure(figsize=(8, 6))\nplt.title(\"n = \" + str(neurons), fontsize=20) # mostrar el numero de neuronas\n# After fit\nnet.fit(X, Y, epochs=300)",
        "detail": "T06.MLP",
        "documentation": {}
    },
    {
        "label": "net",
        "kind": 5,
        "importPath": "T06.MLP",
        "description": "T06.MLP",
        "peekOfCode": "net = DenseNetwork((2, neurons, 1)) # Definimos que tenemos 2 entradas y 1 salida\n# Elegir un csv  -->  [ 'moons', 'blobs', 'circles', 'XOR' ]\nX = np.genfromtxt(\"XOR.csv\", delimiter=',', skip_header=1, usecols=[0,1]).T\nY = np.genfromtxt(\"XOR.csv\", delimiter=',', skip_header=1, usecols=[2]).T\nprint(X.shape, Y.shape)\nplt.figure(figsize=(8, 6))\nplt.title(\"n = \" + str(neurons), fontsize=20) # mostrar el numero de neuronas\n# After fit\nnet.fit(X, Y, epochs=300)\nMLP_binary_classification_2d( plt, X, Y, net )",
        "detail": "T06.MLP",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "T06.MLP",
        "description": "T06.MLP",
        "peekOfCode": "X = np.genfromtxt(\"XOR.csv\", delimiter=',', skip_header=1, usecols=[0,1]).T\nY = np.genfromtxt(\"XOR.csv\", delimiter=',', skip_header=1, usecols=[2]).T\nprint(X.shape, Y.shape)\nplt.figure(figsize=(8, 6))\nplt.title(\"n = \" + str(neurons), fontsize=20) # mostrar el numero de neuronas\n# After fit\nnet.fit(X, Y, epochs=300)\nMLP_binary_classification_2d( plt, X, Y, net )\nplt.grid()\nplt.show()",
        "detail": "T06.MLP",
        "documentation": {}
    },
    {
        "label": "Y",
        "kind": 5,
        "importPath": "T06.MLP",
        "description": "T06.MLP",
        "peekOfCode": "Y = np.genfromtxt(\"XOR.csv\", delimiter=',', skip_header=1, usecols=[2]).T\nprint(X.shape, Y.shape)\nplt.figure(figsize=(8, 6))\nplt.title(\"n = \" + str(neurons), fontsize=20) # mostrar el numero de neuronas\n# After fit\nnet.fit(X, Y, epochs=300)\nMLP_binary_classification_2d( plt, X, Y, net )\nplt.grid()\nplt.show()",
        "detail": "T06.MLP",
        "documentation": {}
    },
    {
        "label": "plt.title(\"n",
        "kind": 5,
        "importPath": "T06.MLP",
        "description": "T06.MLP",
        "peekOfCode": "plt.title(\"n = \" + str(neurons), fontsize=20) # mostrar el numero de neuronas\n# After fit\nnet.fit(X, Y, epochs=300)\nMLP_binary_classification_2d( plt, X, Y, net )\nplt.grid()\nplt.show()",
        "detail": "T06.MLP",
        "documentation": {}
    }
]